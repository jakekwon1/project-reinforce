{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 256ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 132ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 130ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 134ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "Episode: 1, Score: 35.0, Epsilon: 0.99\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[28], line 64\u001b[0m\n\u001b[0;32m     61\u001b[0m         state \u001b[39m=\u001b[39m next_state\n\u001b[0;32m     63\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(memory) \u001b[39m>\u001b[39m batch_size:\n\u001b[1;32m---> 64\u001b[0m             train_model()\n\u001b[0;32m     66\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mEpisode: \u001b[39m\u001b[39m{\u001b[39;00mepisode\u001b[39m \u001b[39m\u001b[39m+\u001b[39m\u001b[39m \u001b[39m\u001b[39m1\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m, Score: \u001b[39m\u001b[39m{\u001b[39;00mscore\u001b[39m}\u001b[39;00m\u001b[39m, Epsilon: \u001b[39m\u001b[39m{\u001b[39;00mepsilon\u001b[39m:\u001b[39;00m\u001b[39m.2f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     68\u001b[0m \u001b[39m# 학습된 모델로 게임 플레이\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[28], line 34\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m()\u001b[0m\n\u001b[0;32m     31\u001b[0m target \u001b[39m=\u001b[39m reward\n\u001b[0;32m     33\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m done:\n\u001b[1;32m---> 34\u001b[0m     target \u001b[39m=\u001b[39m reward \u001b[39m+\u001b[39m gamma \u001b[39m*\u001b[39m np\u001b[39m.\u001b[39mamax(model\u001b[39m.\u001b[39;49mpredict(np\u001b[39m.\u001b[39;49marray([next_state]))[\u001b[39m0\u001b[39m])\n\u001b[0;32m     36\u001b[0m target_f \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mpredict(np\u001b[39m.\u001b[39marray([state]))\n\u001b[0;32m     37\u001b[0m target_f[\u001b[39m0\u001b[39m][action] \u001b[39m=\u001b[39m target\n",
      "File \u001b[1;32mc:\\Users\\jakek\\anaconda3\\envs\\proj4\\lib\\site-packages\\keras\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     66\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\jakek\\anaconda3\\envs\\proj4\\lib\\site-packages\\keras\\engine\\training.py:2378\u001b[0m, in \u001b[0;36mModel.predict\u001b[1;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   2376\u001b[0m callbacks\u001b[39m.\u001b[39mon_predict_begin()\n\u001b[0;32m   2377\u001b[0m batch_outputs \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m-> 2378\u001b[0m \u001b[39mfor\u001b[39;00m _, iterator \u001b[39min\u001b[39;00m data_handler\u001b[39m.\u001b[39menumerate_epochs():  \u001b[39m# Single epoch.\u001b[39;00m\n\u001b[0;32m   2379\u001b[0m     \u001b[39mwith\u001b[39;00m data_handler\u001b[39m.\u001b[39mcatch_stop_iteration():\n\u001b[0;32m   2380\u001b[0m         \u001b[39mfor\u001b[39;00m step \u001b[39min\u001b[39;00m data_handler\u001b[39m.\u001b[39msteps():\n",
      "File \u001b[1;32mc:\\Users\\jakek\\anaconda3\\envs\\proj4\\lib\\site-packages\\keras\\engine\\data_adapter.py:1305\u001b[0m, in \u001b[0;36mDataHandler.enumerate_epochs\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1303\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Yields `(epoch, tf.data.Iterator)`.\"\"\"\u001b[39;00m\n\u001b[0;32m   1304\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_truncate_execution_to_epoch():\n\u001b[1;32m-> 1305\u001b[0m     data_iterator \u001b[39m=\u001b[39m \u001b[39miter\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset)\n\u001b[0;32m   1306\u001b[0m     \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_initial_epoch, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_epochs):\n\u001b[0;32m   1307\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_insufficient_data:  \u001b[39m# Set by `catch_stop_iteration`.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\jakek\\anaconda3\\envs\\proj4\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py:505\u001b[0m, in \u001b[0;36mDatasetV2.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    503\u001b[0m \u001b[39mif\u001b[39;00m context\u001b[39m.\u001b[39mexecuting_eagerly() \u001b[39mor\u001b[39;00m ops\u001b[39m.\u001b[39minside_function():\n\u001b[0;32m    504\u001b[0m   \u001b[39mwith\u001b[39;00m ops\u001b[39m.\u001b[39mcolocate_with(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_variant_tensor):\n\u001b[1;32m--> 505\u001b[0m     \u001b[39mreturn\u001b[39;00m iterator_ops\u001b[39m.\u001b[39;49mOwnedIterator(\u001b[39mself\u001b[39;49m)\n\u001b[0;32m    506\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    507\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39m`tf.data.Dataset` only supports Python-style \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    508\u001b[0m                      \u001b[39m\"\u001b[39m\u001b[39miteration in eager mode or within tf.function.\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\jakek\\anaconda3\\envs\\proj4\\lib\\site-packages\\tensorflow\\python\\data\\ops\\iterator_ops.py:713\u001b[0m, in \u001b[0;36mOwnedIterator.__init__\u001b[1;34m(self, dataset, components, element_spec)\u001b[0m\n\u001b[0;32m    709\u001b[0m   \u001b[39mif\u001b[39;00m (components \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m element_spec \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m):\n\u001b[0;32m    710\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    711\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mWhen `dataset` is provided, `element_spec` and `components` must \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    712\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mnot be specified.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m--> 713\u001b[0m   \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_create_iterator(dataset)\n\u001b[0;32m    715\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_next_call_count \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\jakek\\anaconda3\\envs\\proj4\\lib\\site-packages\\tensorflow\\python\\data\\ops\\iterator_ops.py:735\u001b[0m, in \u001b[0;36mOwnedIterator._create_iterator\u001b[1;34m(self, dataset)\u001b[0m\n\u001b[0;32m    731\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_flat_output_shapes \u001b[39m=\u001b[39m structure\u001b[39m.\u001b[39mget_flat_tensor_shapes(\n\u001b[0;32m    732\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_element_spec)\n\u001b[0;32m    733\u001b[0m \u001b[39mwith\u001b[39;00m ops\u001b[39m.\u001b[39mcolocate_with(ds_variant):\n\u001b[0;32m    734\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_iterator_resource \u001b[39m=\u001b[39m (\n\u001b[1;32m--> 735\u001b[0m       gen_dataset_ops\u001b[39m.\u001b[39;49manonymous_iterator_v3(\n\u001b[0;32m    736\u001b[0m           output_types\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_flat_output_types,\n\u001b[0;32m    737\u001b[0m           output_shapes\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_flat_output_shapes))\n\u001b[0;32m    738\u001b[0m   \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m context\u001b[39m.\u001b[39mexecuting_eagerly():\n\u001b[0;32m    739\u001b[0m     \u001b[39m# Add full type information to the graph so host memory types inside\u001b[39;00m\n\u001b[0;32m    740\u001b[0m     \u001b[39m# variants stay on CPU, e.g, ragged string tensors.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    744\u001b[0m     \u001b[39m# type inference (esp. cross-function type inference) instead of\u001b[39;00m\n\u001b[0;32m    745\u001b[0m     \u001b[39m# setting the full type information manually.\u001b[39;00m\n\u001b[0;32m    746\u001b[0m     fulltype \u001b[39m=\u001b[39m type_utils\u001b[39m.\u001b[39miterator_full_type_from_spec(\n\u001b[0;32m    747\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_element_spec)\n",
      "File \u001b[1;32mc:\\Users\\jakek\\anaconda3\\envs\\proj4\\lib\\site-packages\\tensorflow\\python\\ops\\gen_dataset_ops.py:202\u001b[0m, in \u001b[0;36manonymous_iterator_v3\u001b[1;34m(output_types, output_shapes, name)\u001b[0m\n\u001b[0;32m    200\u001b[0m \u001b[39mif\u001b[39;00m tld\u001b[39m.\u001b[39mis_eager:\n\u001b[0;32m    201\u001b[0m   \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 202\u001b[0m     _result \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_FastPathExecute(\n\u001b[0;32m    203\u001b[0m       _ctx, \u001b[39m\"\u001b[39;49m\u001b[39mAnonymousIteratorV3\u001b[39;49m\u001b[39m\"\u001b[39;49m, name, \u001b[39m\"\u001b[39;49m\u001b[39moutput_types\u001b[39;49m\u001b[39m\"\u001b[39;49m, output_types,\n\u001b[0;32m    204\u001b[0m       \u001b[39m\"\u001b[39;49m\u001b[39moutput_shapes\u001b[39;49m\u001b[39m\"\u001b[39;49m, output_shapes)\n\u001b[0;32m    205\u001b[0m     \u001b[39mreturn\u001b[39;00m _result\n\u001b[0;32m    206\u001b[0m   \u001b[39mexcept\u001b[39;00m _core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import random\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# 게임 환경 생성\n",
    "env = gym.make('CartPole-v1')\n",
    "\n",
    "# Q-Network 모델 생성\n",
    "model = Sequential()\n",
    "model.add(Dense(24, input_shape=(4,), activation='relu'))\n",
    "model.add(Dense(24, activation='relu'))\n",
    "model.add(Dense(2, activation='linear'))\n",
    "model.compile(loss='mse', optimizer=Adam(lr=0.001))\n",
    "\n",
    "# 학습 파라미터 설정\n",
    "epsilon = 1.0  # 탐험 비율\n",
    "epsilon_decay = 0.995  # 탐험 비율 감소율\n",
    "gamma = 0.95  # 할인 계수\n",
    "batch_size = 32  # 미니배치 크기\n",
    "memory = []  # 리플레이 메모리\n",
    "\n",
    "# DQN 학습 함수\n",
    "def train_model():\n",
    "    global epsilon\n",
    "    batch = random.sample(memory, batch_size)\n",
    "\n",
    "    for state, action, reward, next_state, done in batch:\n",
    "        target = reward\n",
    "\n",
    "        if not done:\n",
    "            target = reward + gamma * np.amax(model.predict(np.array([next_state]))[0])\n",
    "\n",
    "        target_f = model.predict(np.array([state]))\n",
    "        target_f[0][action] = target\n",
    "\n",
    "        model.fit(np.array([state]), target_f, epochs=1, verbose=0)\n",
    "\n",
    "    if epsilon > 0.01:\n",
    "        epsilon *= epsilon_decay\n",
    "\n",
    "# 게임 실행 및 학습\n",
    "episodes = 1000\n",
    "\n",
    "for episode in range(episodes):\n",
    "    state = env.reset()[0]\n",
    "    done = False\n",
    "    score = 0\n",
    "\n",
    "    while not done:\n",
    "        if random.uniform(0, 1) < epsilon:\n",
    "            action = env.action_space.sample()\n",
    "        else:\n",
    "            action = np.argmax(model.predict(np.array([state]))[0])\n",
    "\n",
    "        next_state, reward, done, _, _= env.step(action)\n",
    "        score += reward\n",
    "        memory.append((state, action, reward, next_state, done))\n",
    "        state = next_state\n",
    "\n",
    "        if len(memory) > batch_size:\n",
    "            train_model()\n",
    "\n",
    "    print(f\"Episode: {episode + 1}, Score: {score}, Epsilon: {epsilon:.2f}\")\n",
    "\n",
    "# 학습된 모델로 게임 플레이\n",
    "state = env.reset()\n",
    "done = False\n",
    "score = 0\n",
    "\n",
    "while not done:\n",
    "    action = np.argmax(model.predict(np.array([state]))[0])\n",
    "    next_state, reward, done, _ = env.step(action)\n",
    "    state = next_state\n",
    "    score += reward\n",
    "\n",
    "env.close()\n",
    "print(f\"Final Score: {score}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] Unable to open file (unable to open file: name = 'dqn_cartpole.h5', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[29], line 14\u001b[0m\n\u001b[0;32m     12\u001b[0m model\u001b[39m.\u001b[39madd(Dense(\u001b[39m24\u001b[39m, activation\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mrelu\u001b[39m\u001b[39m'\u001b[39m))\n\u001b[0;32m     13\u001b[0m model\u001b[39m.\u001b[39madd(Dense(\u001b[39m2\u001b[39m, activation\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mlinear\u001b[39m\u001b[39m'\u001b[39m))\n\u001b[1;32m---> 14\u001b[0m model\u001b[39m.\u001b[39;49mload_weights(\u001b[39m'\u001b[39;49m\u001b[39mdqn_cartpole.h5\u001b[39;49m\u001b[39m'\u001b[39;49m)  \u001b[39m# 학습된 모델 불러오기\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \u001b[39m# 게임 플레이\u001b[39;00m\n\u001b[0;32m     17\u001b[0m state \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39mreset()\n",
      "File \u001b[1;32mc:\\Users\\jakek\\anaconda3\\envs\\proj4\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\Users\\jakek\\anaconda3\\envs\\proj4\\lib\\site-packages\\h5py\\_hl\\files.py:567\u001b[0m, in \u001b[0;36mFile.__init__\u001b[1;34m(self, name, mode, driver, libver, userblock_size, swmr, rdcc_nslots, rdcc_nbytes, rdcc_w0, track_order, fs_strategy, fs_persist, fs_threshold, fs_page_size, page_buf_size, min_meta_keep, min_raw_keep, locking, alignment_threshold, alignment_interval, meta_block_size, **kwds)\u001b[0m\n\u001b[0;32m    558\u001b[0m     fapl \u001b[39m=\u001b[39m make_fapl(driver, libver, rdcc_nslots, rdcc_nbytes, rdcc_w0,\n\u001b[0;32m    559\u001b[0m                      locking, page_buf_size, min_meta_keep, min_raw_keep,\n\u001b[0;32m    560\u001b[0m                      alignment_threshold\u001b[39m=\u001b[39malignment_threshold,\n\u001b[0;32m    561\u001b[0m                      alignment_interval\u001b[39m=\u001b[39malignment_interval,\n\u001b[0;32m    562\u001b[0m                      meta_block_size\u001b[39m=\u001b[39mmeta_block_size,\n\u001b[0;32m    563\u001b[0m                      \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n\u001b[0;32m    564\u001b[0m     fcpl \u001b[39m=\u001b[39m make_fcpl(track_order\u001b[39m=\u001b[39mtrack_order, fs_strategy\u001b[39m=\u001b[39mfs_strategy,\n\u001b[0;32m    565\u001b[0m                      fs_persist\u001b[39m=\u001b[39mfs_persist, fs_threshold\u001b[39m=\u001b[39mfs_threshold,\n\u001b[0;32m    566\u001b[0m                      fs_page_size\u001b[39m=\u001b[39mfs_page_size)\n\u001b[1;32m--> 567\u001b[0m     fid \u001b[39m=\u001b[39m make_fid(name, mode, userblock_size, fapl, fcpl, swmr\u001b[39m=\u001b[39mswmr)\n\u001b[0;32m    569\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(libver, \u001b[39mtuple\u001b[39m):\n\u001b[0;32m    570\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_libver \u001b[39m=\u001b[39m libver\n",
      "File \u001b[1;32mc:\\Users\\jakek\\anaconda3\\envs\\proj4\\lib\\site-packages\\h5py\\_hl\\files.py:231\u001b[0m, in \u001b[0;36mmake_fid\u001b[1;34m(name, mode, userblock_size, fapl, fcpl, swmr)\u001b[0m\n\u001b[0;32m    229\u001b[0m     \u001b[39mif\u001b[39;00m swmr \u001b[39mand\u001b[39;00m swmr_support:\n\u001b[0;32m    230\u001b[0m         flags \u001b[39m|\u001b[39m\u001b[39m=\u001b[39m h5f\u001b[39m.\u001b[39mACC_SWMR_READ\n\u001b[1;32m--> 231\u001b[0m     fid \u001b[39m=\u001b[39m h5f\u001b[39m.\u001b[39;49mopen(name, flags, fapl\u001b[39m=\u001b[39;49mfapl)\n\u001b[0;32m    232\u001b[0m \u001b[39melif\u001b[39;00m mode \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mr+\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[0;32m    233\u001b[0m     fid \u001b[39m=\u001b[39m h5f\u001b[39m.\u001b[39mopen(name, h5f\u001b[39m.\u001b[39mACC_RDWR, fapl\u001b[39m=\u001b[39mfapl)\n",
      "File \u001b[1;32mh5py\\_objects.pyx:54\u001b[0m, in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mh5py\\_objects.pyx:55\u001b[0m, in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mh5py\\h5f.pyx:106\u001b[0m, in \u001b[0;36mh5py.h5f.open\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] Unable to open file (unable to open file: name = 'dqn_cartpole.h5', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "# 게임 환경 생성\n",
    "env = gym.make('CartPole-v1')\n",
    "\n",
    "# Q-Network 모델 생성\n",
    "model = Sequential()\n",
    "model.add(Dense(24, input_shape=(4,), activation='relu'))\n",
    "model.add(Dense(24, activation='relu'))\n",
    "model.add(Dense(2, activation='linear'))\n",
    "model.load_weights('dqn_cartpole.h5')  # 학습된 모델 불러오기\n",
    "\n",
    "# 게임 플레이\n",
    "state = env.reset()\n",
    "done = False\n",
    "score = 0\n",
    "\n",
    "while not done:\n",
    "    action = np.argmax(model.predict(np.array([state]))[0])\n",
    "    next_state, reward, done, _ = env.step(action)\n",
    "    state = next_state\n",
    "    score += reward\n",
    "\n",
    "env.close()\n",
    "print(f\"Final Score: {score}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jakek\\anaconda3\\envs\\proj4\\lib\\site-packages\\gym\\envs\\registration.py:555: UserWarning: \u001b[33mWARN: The environment CartPole-v0 is out of date. You should consider upgrading to version `v1`.\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAh8AAAEeCAYAAAAq6XfpAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAqeElEQVR4nO3deXRUVdb38V8lUJUASYUpCYEQJhXCYJDJgAotadI8SIM4oDiAY6tBRPpRwW7EoQGnBhURHMGhaVpsQEUFMSI+0oBAg4IDIiIgkgBqBgIkIbXfP1yp1yIVTCDcpOL3s9Zdi9r3VN196oaqXafOueUyMxMAAIBDwqo7AQAA8NtC8QEAABxF8QEAABxF8QEAABxF8QEAABxF8QEAABxF8QEAABxF8QEAABxF8QEAABxF8QGEiA8++EAul0sffPBBdafym+RyuXTvvfdWdxpArUDxgVph7ty5crlc5W5r1qyp7hRrvc8//1z33nuvvv3222rLYd68eXrssceq7fgAKqZOdScAVKX7779frVu3LhNv165dNWTz2/L555/rvvvuU79+/dSqVatqyWHevHnasmWLxo4dWy3HB1AxFB+oVQYOHKju3btXdxr4FWamI0eOKDIysrpTCRkFBQWqX79+dacBVAm+dsFvyqRJkxQWFqbMzMyA+I033ii3261PPvlEklRUVKR77rlH3bp1k9frVf369XXuuedqxYoVAff79ttv5XK59Oijj2rmzJlq06aN6tWrpwEDBmj37t0yMz3wwANq0aKFIiMjNWTIEP34448Bj9GqVStdcMEFevfdd5WSkqKIiAglJydr4cKFFerT2rVr9Yc//EFer1f16tVT3759tWrVqgrdt7CwUJMmTVK7du3k8XiUmJioO++8U4WFhf42I0eOVEREhL744ouA+6anp6thw4b6/vvvNXfuXF1yySWSpN/97nf+r7tK56eU9nHZsmXq3r27IiMj9fTTT0uS5syZo/PPP1+xsbHyeDxKTk7WrFmzgub7zjvvqG/fvoqKilJ0dLR69OihefPmSZL69eunt956Szt37vQf/5cjMBXpa2m722+/XU2bNlVUVJT++Mc/6rvvvqvQ8ylJM2bMUMeOHVWvXj01bNhQ3bt39+dYas+ePbruuuuUkJAgj8ej1q1b6+abb1ZRUZGk//814sqVK3XLLbcoNjZWLVq0CHgezj33XNWvX19RUVEaNGiQPvvsszK5fPnll7r44ovVqFEjRUREqHv37nrjjTcC2pQea9WqVRo3bpyaNm2q+vXr68ILL9T+/fsr3G+gUgyoBebMmWOS7L333rP9+/cHbAcOHPC3Kyoqsq5du1pSUpLl5eWZmdnSpUtNkj3wwAP+dvv377dmzZrZuHHjbNasWfbwww/bGWecYXXr1rWNGzf62+3YscMkWUpKiiUnJ9u0adPsr3/9q7ndbjv77LPt7rvvtt69e9sTTzxhY8aMMZfLZddcc01A7klJSXb66adbTEyMjR8/3qZNm2adO3e2sLAwe/fdd/3tVqxYYZJsxYoV/lhmZqa53W5LTU21v//97zZ9+nTr0qWLud1uW7t27XGfs5KSEhswYIDVq1fPxo4da08//bSNHj3a6tSpY0OGDPG3++mnn6xFixbWo0cPO3r0qJmZzZ492yTZyy+/bGZm27dvtzFjxpgku/vuu+3ll1+2l19+2bKysvx9bNeunTVs2NDGjx9vs2fP9vejR48eNmrUKJs+fbrNmDHDBgwYYJLsySefLHOOXS6XderUySZPnmwzZ86066+/3q666iozM3v33XctJSXFmjRp4j/+okWLKtVXM7Mrr7zSJNmIESPsySeftGHDhlmXLl1Mkk2aNOm4z+kzzzxjkuziiy+2p59+2h5//HG77rrrbMyYMf42e/bssYSEBH8us2fPtokTJ1qHDh3sp59+8vdVkiUnJ1vfvn1txowZ9uCDD5qZ2UsvvWQul8v+8Ic/2IwZM+yhhx6yVq1aWUxMjO3YscN/nC1btpjX67Xk5GR76KGH7Mknn7TzzjvPXC6XLVy4MOB5lWRdu3a1888/32bMmGF//vOfLTw83C699NLj9hc4URQfqBVKX0CDbR6PJ6Dt5s2bze122/XXX28//fSTNW/e3Lp3727FxcX+NkePHrXCwsKA+/30008WFxdn1157rT9WWnw0bdrUcnJy/PEJEyaYJDvzzDMDHvfyyy83t9ttR44c8ceSkpJMkv373//2x3Jzc61Zs2bWtWtXf+zY4sPn89lpp51m6enp5vP5/O0OHTpkrVu3tt///vfHfc5efvllCwsLs//7v/8LiJcWFqtWrfLHli1bZpLsb3/7m33zzTfWoEEDGzp0aMD9FixYUKY4OraPS5cuLbPv0KFDZWLp6enWpk0b/+2cnByLioqyXr162eHDhwPa/rLvgwYNsqSkpBPu66ZNm0yS3XLLLQHtRowYUaHiY8iQIdaxY8fjtrn66qstLCzM1q1bV2ZfaV9K/57POeccf8FnZpafn28xMTF2ww03BNwvKyvLvF5vQLx///7WuXPngL81n89nvXv3ttNOO80fKz1WWlpawHN5++23W3h4eMDfNVBV+NoFtcrMmTO1fPnygO2dd94JaNOpUyfdd999eu6555Senq4DBw7oxRdfVJ06/38KVHh4uNxutyTJ5/Ppxx9/1NGjR9W9e3f997//LXPcSy65RF6v13+7V69ekqQrr7wy4HF79eqloqIi7dmzJ+D+CQkJuvDCC/23o6OjdfXVV2vjxo3KysoK2tdNmzZp27ZtGjFihH744QcdOHBABw4cUEFBgfr3768PP/xQPp+v3OdqwYIF6tChg9q3b++/74EDB3T++edLUsBXTAMGDNCf/vQn3X///Ro2bJgiIiL8X5tUVOvWrZWenl4m/st5H7m5uTpw4ID69u2rb775Rrm5uZKk5cuXKz8/X+PHj1dERETA/V0u168eu6J9ffvttyVJY8aMCbh/RSewxsTE6LvvvtO6deuC7vf5fFq8eLEGDx4cdG7SsX254YYbFB4e7r+9fPly5eTk6PLLLw/oR3h4uHr16uXvx48//qj3339fl156qfLz8/3tfvjhB6Wnp2vbtm1l/gZvvPHGgOOfe+65Kikp0c6dOyvUd6AymHCKWqVnz54VmnB6xx13aP78+fr44481ZcoUJScnl2nz4osv6u9//7u+/PJLFRcX++PBVtO0bNky4HZpIZKYmBg0/tNPPwXE27VrV+aN5/TTT5f087yS+Pj4Msfctm2bpJ/nZJQnNzdXDRs2DLpv27Zt+uKLL9S0adOg+/ft2xdw+9FHH9Xrr7+uTZs2ad68eYqNjS33uMEEe94kadWqVZo0aZJWr16tQ4cOlcnf6/Vq+/btkn4uHE9ERfu6c+dOhYWFqW3btgH7zzjjjAod56677tJ7772nnj17ql27dhowYIBGjBihPn36SJL279+vvLy8Cvfj2Oes9JyXFk3Hio6OliR9/fXXMjNNnDhREydODNp23759at68uf/2sX/DpX83x/6tAlWB4gO/Sd98843/hXzz5s1l9r/yyisaNWqUhg4dqjvuuEOxsbEKDw/X1KlT/W+Ev/TLT6cViZvZSWT/s9JRjUceeUQpKSlB2zRo0OC49+/cubOmTZsWdP+xhdPGjRv9b9KbN2/W5ZdfXql8g61s2b59u/r376/27dtr2rRpSkxMlNvt1ttvv63p06cfd+SmMirb1xPVoUMHbd26VUuWLNHSpUv173//W0899ZTuuece3XfffZV+vGOfs9Ln4+WXXw5akJaOspW2+9///d+go01S2eXnp/JvFTgWxQd+c3w+n0aNGqXo6GiNHTtWU6ZM0cUXX6xhw4b527z22mtq06aNFi5cGDAiMWnSpFOSU+kn1V8e66uvvpKkcq+ZUfrpPDo6WmlpaZU+Ztu2bfXJJ5+of//+v/rVRUFBga655holJyerd+/eevjhh3XhhReqR48e/jYV+frjWG+++aYKCwv1xhtvBHzyPnZVUWlft2zZctxrtpSXQ0X7mpSUJJ/Pp+3btweMdmzdurVC/ZGk+vXra/jw4Ro+fLiKioo0bNgwTZ48WRMmTFDTpk0VHR2tLVu2VPjxju2HJMXGxh73nLdp00aSVLdu3RP62wBONeZ84Ddn2rRp+s9//qNnnnlGDzzwgHr37q2bb75ZBw4c8Lcp/RT4y099a9eu1erVq09JTt9//70WLVrkv52Xl6eXXnpJKSkpQT/hSlK3bt3Utm1bPfroozp48GCZ/b+2TPLSSy/Vnj179Oyzz5bZd/jwYRUUFPhv33XXXdq1a5defPFFTZs2Ta1atdLIkSMDlqmWXoMiJyfnuMf9pWDPc25urubMmRPQbsCAAYqKitLUqVN15MiRgH2/vG/9+vX980ROpK8DBw6UJD3xxBMBbSp61dQffvgh4Lbb7VZycrLMTMXFxQoLC9PQoUP15ptvav369WXu/2ujDOnp6YqOjtaUKVMCvgosVXrOY2Nj1a9fPz399NPau3dvue2A6sLIB2qVd955R19++WWZeO/evdWmTRt98cUXmjhxokaNGqXBgwdL+vk6BykpKbrlllv06quvSpIuuOACLVy4UBdeeKEGDRqkHTt2aPbs2UpOTg76Rn+yTj/9dF133XVat26d4uLi9MILLyg7O7vMm/AvhYWF6bnnntPAgQPVsWNHXXPNNWrevLn27NmjFStWKDo6Wm+++Wa597/qqqv06quv6qabbtKKFSvUp08flZSU6Msvv9Srr77qvybH+++/r6eeekqTJk3SWWedJenna3P069dPEydO1MMPPyxJSklJUXh4uB566CHl5ubK4/H4r99RngEDBsjtdmvw4MH605/+pIMHD+rZZ59VbGxswJtmdHS0pk+fruuvv149evTQiBEj1LBhQ33yySc6dOiQXnzxRUk/F2T/+te/NG7cOPXo0UMNGjTQ4MGDK9zXlJQUXX755XrqqaeUm5ur3r17KzMzU19//XWFzuOAAQMUHx+vPn36KC4uTl988YWefPJJDRo0SFFRUZKkKVOm6N1331Xfvn114403qkOHDtq7d68WLFigjz76SDExMeU+fnR0tGbNmqWrrrpKZ511li677DI1bdpUu3bt0ltvvaU+ffroySeflPTz5OtzzjlHnTt31g033KA2bdooOztbq1ev1nfffee/pg1QLaptnQ1QhY631FaSzZkzx44ePWo9evSwFi1alFk++Pjjj5sk+9e//mVmPy9JnDJliiUlJZnH47GuXbvakiVLbOTIkQFLOUuX2j7yyCMBj1e6LHbBggVB8/zlMsukpCQbNGiQLVu2zLp06WIej8fat29f5r7BrvNhZrZx40YbNmyYNW7c2DwejyUlJdmll15qmZmZv/q8FRUV2UMPPWQdO3Y0j8djDRs2tG7dutl9991nubm5lpeXZ0lJSXbWWWcFLBk2+3kpZlhYmK1evdofe/bZZ61NmzYWHh4ekGtpH4N54403rEuXLhYREWGtWrWyhx56yF544QWTFHDditK2vXv3tsjISIuOjraePXvaP//5T//+gwcP2ogRIywmJsYkBZyrX+trqcOHD9uYMWOscePGVr9+fRs8eLDt3r27Qkttn376aTvvvPP856Jt27Z2xx13BDy+mdnOnTvt6quvtqZNm5rH47E2bdpYRkaGf3l3sL+TX1qxYoWlp6eb1+u1iIgIa9u2rY0aNcrWr18f0G779u129dVXW3x8vNWtW9eaN29uF1xwgb322mv+NuUdq7y/N6AquMyYTQRUp1atWqlTp05asmRJdacCAI5gzgcAAHAUxQcAAHAUxQcAAHAUcz4AAICjGPkAAACOovgAAACOOmUXGZs5c6YeeeQRZWVl6cwzz9SMGTPUs2fPX72fz+fT999/r6ioqBO6XDMAAHCemSk/P18JCQkKC/uVsY1TcfGQ+fPnm9vtthdeeME+++wzu+GGGywmJsays7N/9b6lF/NhY2NjY2NjC71t9+7dv/pef0omnPbq1Us9evTwX+bX5/MpMTFRt956q8aPH3/c++bm5iomJka7d+/2/zw0AACo2fLy8pSYmKicnBx5vd7jtq3yr12Kioq0YcMGTZgwwR8LCwtTWlpa0B/lKiwsDPhxqvz8fEk//4YBxQcAAKGlIlMmqnzC6YEDB1RSUqK4uLiAeFxcnLKyssq0nzp1qrxer39LTEys6pQAAEANUu2rXSZMmKDc3Fz/tnv37upOCQAAnEJV/rVLkyZNFB4eruzs7IB4dna24uPjy7T3eDzyeDxVnQYAAKihqnzkw+12q1u3bsrMzPTHfD6fMjMzlZqaWtWHAwAAIeaUXOdj3LhxGjlypLp3766ePXvqscceU0FBga655ppTcTgAABBCTknxMXz4cO3fv1/33HOPsrKylJKSoqVLl5aZhAoAAH57atwPy+Xl5cnr9So3N5eltgAAhIjKvH+fssurA6h9fEeLKhUPJtwdGTTuCgs/oZwAhJ5qX2oLAAB+Wyg+AACAoyg+AACAoyg+AACAoyg+AACAo1jtAqDC9n22Imh8z7o3ysRc4cFfXk5LvyVoPCrhjBNPDEBIYeQDAAA4iuIDAAA4iuIDAAA4iuIDAAA4igmnACrMV1wYNF58OK9MrLzLpR8tPFylOQEIPYx8AAAAR1F8AAAAR1F8AAAAR1F8AAAAR1F8AAAAR7HaBUDFucoJu4J8jgkWk2QlRVWYEIBQxMgHAABwFMUHAABwFMUHAABwFMUHAABwFBNOAVRY3XoNg+8INrnUSoI2LTr4YxVmBCAUMfIBAAAcRfEBAAAcRfEBAAAcRfEBAAAcRfEBAAAcxWoXABVWx1M/aNzlKnvddfMFfwwfl1cHfvMY+QAAAI6i+AAAAI6i+AAAAI6i+AAAAI5iwimACgurUzf4jrLzTY+jUo0B1EKMfAAAAEdRfAAAAEdRfAAAAEdRfAAAAEdRfAAAAEex2gVAhbnCy1ntUokVLGZWNckACFmMfAAAAEdRfAAAAEdRfAAAAEdRfAAAAEdVuvj48MMPNXjwYCUkJMjlcmnx4sUB+81M99xzj5o1a6bIyEilpaVp27ZtVZUvgFDhCr75jhYH3QD8dlS6+CgoKNCZZ56pmTNnBt3/8MMP64knntDs2bO1du1a1a9fX+np6Tpy5MhJJwsAAEJfpZfaDhw4UAMHDgy6z8z02GOP6a9//auGDBkiSXrppZcUFxenxYsX67LLLju5bAEAQMir0jkfO3bsUFZWltLS0vwxr9erXr16afXq1UHvU1hYqLy8vIANAADUXlVafGRlZUmS4uLiAuJxcXH+fceaOnWqvF6vf0tMTKzKlAAAQA1T7atdJkyYoNzcXP+2e/fu6k4JAACcQlV6efX4+HhJUnZ2tpo1a+aPZ2dnKyUlJeh9PB6PPB5PVaYB4JSpzKXRg19y3VdSVDWpAAhZVTry0bp1a8XHxyszM9Mfy8vL09q1a5WamlqVhwIAACGq0iMfBw8e1Ndff+2/vWPHDm3atEmNGjVSy5YtNXbsWP3tb3/TaaedptatW2vixIlKSEjQ0KFDqzJvAAAQoipdfKxfv16/+93v/LfHjRsnSRo5cqTmzp2rO++8UwUFBbrxxhuVk5Ojc845R0uXLlVERETVZQ0AAEJWpYuPfv36HfcnsV0ul+6//37df//9J5UYAAConap0wimA2i28bmTQuCu87EuJ+UqCti0uyK3SnACEnmpfagsAAH5bKD4AAICjKD4AAICjKD4AAICjKD4AAICjWO0CoMLqRNQPGg+r4y4T8xUXBm1bUlhQpTkBCD2MfAAAAEdRfAAAAEdRfAAAAEdRfAAAAEcx4RRAhQW7jLokuVyV+BzjclVRNgBCFSMfAADAURQfAADAURQfAADAURQfAADAURQfAADAUax2AVBhYeWtdgkL9jnGTm0yAEIWIx8AAMBRFB8AAMBRFB8AAMBRFB8AAMBRTDgFUAnlXRo9WDx4W1/J0SrLBkBoYuQDAAA4iuIDAAA4iuIDAAA4iuIDAAA4iuIDAAA4itUuABxl5a12sSCXY3eVt7oGQChj5AMAADiK4gMAADiK4gMAADiK4gMAADiKCacAKszlCv55xRVW8ZeSkuIjQeO+kuIysbA67go/LoDQwcgHAABwFMUHAABwFMUHAABwFMUHAABwFMUHAABwFKtdAFRYWN2IoPG69aLLxArz9gVte/TIwaBx39GissdjtQtQKzHyAQAAHEXxAQAAHEXxAQAAHEXxAQAAHFWp4mPq1Knq0aOHoqKiFBsbq6FDh2rr1q0BbY4cOaKMjAw1btxYDRo00EUXXaTs7OwqTRpA9XCFhQXfwuuU2QCgPJUqPlauXKmMjAytWbNGy5cvV3FxsQYMGKCCggJ/m9tvv11vvvmmFixYoJUrV+r777/XsGHDqjxxAAAQmir18WTp0qUBt+fOnavY2Fht2LBB5513nnJzc/X8889r3rx5Ov/88yVJc+bMUYcOHbRmzRqdffbZVZc5AAAISSc15yM3N1eS1KhRI0nShg0bVFxcrLS0NH+b9u3bq2XLllq9enXQxygsLFReXl7ABgAAaq8TLj58Pp/Gjh2rPn36qFOnTpKkrKwsud1uxcTEBLSNi4tTVlZW0MeZOnWqvF6vf0tMTDzRlAAAQAg44eIjIyNDW7Zs0fz5808qgQkTJig3N9e/7d69+6QeDwAA1GwnNCV99OjRWrJkiT788EO1aNHCH4+Pj1dRUZFycnICRj+ys7MVHx8f9LE8Ho88Hs+JpAGghggLrxskao7nASA0VGrkw8w0evRoLVq0SO+//75at24dsL9bt26qW7euMjMz/bGtW7dq165dSk1NrZqMAQBASKvUyEdGRobmzZun119/XVFRUf55HF6vV5GRkfJ6vbruuus0btw4NWrUSNHR0br11luVmprKShcAACCpksXHrFmzJEn9+vULiM+ZM0ejRo2SJE2fPl1hYWG66KKLVFhYqPT0dD311FNVkiwAAAh9lSo+zH79O9yIiAjNnDlTM2fOPOGkAABA7cVvuwAAAEfxAwwATprL5ap4Y/OVEw4eB1D7MPIBAAAcRfEBAAAcRfEBAAAcRfEBAAAcxYRTAKdI8Emo5U0s9ZUUn8pkANQgjHwAAABHUXwAAABHUXwAAABHUXwAAABHUXwAAABHsdoFwMmrxOXVzVcSNO47WlRV2QCo4Rj5AAAAjqL4AAAAjqL4AAAAjqL4AAAAjmLCKYCT5m7QuMJtS4oLg8aLD+eViUU2bHbCOQGouRj5AAAAjqL4AAAAjqL4AAAAjqL4AAAAjqL4AAAAjmK1C4CTFl43ohKtrZxwOXEAtQ4jHwAAwFEUHwAAwFEUHwAAwFEUHwAAwFFMOAVw0sLquINEmUAKIDhGPgAAgKMoPgAAgKMoPgAAgKMoPgAAgKMoPgAAgKNY7QLgpIWFh5cNVnKxi5mvapIBUOMx8gEAABxF8QEAABxF8QEAABxF8QEAABzFhFMAJy0srBKfY6ycmai+o1WTDIAaj5EPAADgKIoPAADgKIoPAADgKIoPAADgqEoVH7NmzVKXLl0UHR2t6Ohopaam6p133vHvP3LkiDIyMtS4cWM1aNBAF110kbKzs6s8aQAAELoqtdqlRYsWevDBB3XaaafJzPTiiy9qyJAh2rhxozp27Kjbb79db731lhYsWCCv16vRo0dr2LBhWrVq1anKH0AQxcXFQeO5ubmn5HgFBQVlYmGu4KtaXK7gl1HP+WF/mVhxvQMnl1g56tWrV6k4gKpVqeJj8ODBAbcnT56sWbNmac2aNWrRooWef/55zZs3T+eff74kac6cOerQoYPWrFmjs88+u+qyBgAAIeuE53yUlJRo/vz5KigoUGpqqjZs2KDi4mKlpaX527Rv314tW7bU6tWry32cwsJC5eXlBWwAAKD2qnTxsXnzZjVo0EAej0c33XSTFi1apOTkZGVlZcntdismJiagfVxcnLKyssp9vKlTp8rr9fq3xMTESncCAACEjkoXH2eccYY2bdqktWvX6uabb9bIkSP1+eefn3ACEyZMUG5urn/bvXv3CT8WAACo+Sp9eXW326127dpJkrp166Z169bp8ccf1/Dhw1VUVKScnJyA0Y/s7GzFx8eX+3gej0cej6fymQMo15o1a4LGhw0bdkqO94duLcrEMi5KC9JSKlHw/++PTr6nTGz+yq9OLrFy3HnnnUHjd9xxxyk5HoBAJ32dD5/Pp8LCQnXr1k1169ZVZmamf9/WrVu1a9cupaamnuxhAABALVGpkY8JEyZo4MCBatmypfLz8zVv3jx98MEHWrZsmbxer6677jqNGzdOjRo1UnR0tG699Valpqay0gUAAPhVqvjYt2+frr76au3du1der1ddunTRsmXL9Pvf/16SNH36dIWFhemiiy5SYWGh0tPT9dRTT52SxAEAQGiqVPHx/PPPH3d/RESEZs6cqZkzZ55UUgAAoPbit10AAICjKr3aBUDNV1RUFDR+4MCpuVz5V98nlIn9J+fCoG1LXA2Cxr/N/bJM7MCB/5xcYuU4ePDgKXlcABXDyAcAAHAUxQcAAHAUxQcAAHAUxQcAAHAUE06BWqhOHWf/ax8+6g6ShDdo2zphEUHjJWHB258KTj8/AAIx8gEAABxF8QEAABxF8QEAABxF8QEAABxF8QEAABxVY6d8b9myRQ0aBL8MM4Dj27Ztm6PH+3F/2Uuj/9+ySUHbHlX9oPGsb9+v0pyOZ+/evUHjn376qWM5ALVNZX62gJEPAADgKIoPAADgKIoPAADgKIoPAADgqBo74bRJkyaKioqq7jSAkBQTE+Po8fYcKDvRbM+yfzuaQ2XUrx980mvTpk0dzgSoPSIigv90QjCMfAAAAEdRfAAAAEdRfAAAAEdRfAAAAEdRfAAAAEfV2NUu8fHxio6Oru40gJDUpEmT6k6hRitvJV2zZs0czgSoPcpbRRYMIx8AAMBRFB8AAMBRFB8AAMBRFB8AAMBRNXbCKYATd/To0epOoUYrLi6u7hSA3zRGPgAAgKMoPgAAgKMoPgAAgKMoPgAAgKMoPgAAgKNY7QLUQuVdXj0tLc3hTGqm008/vbpTAH7TGPkAAACOovgAAACOovgAAACOovgAAACOYsIpUAulpKQEjS9fvtzZRAAgCEY+AACAoyg+AACAoyg+AACAoyg+AACAo2rchFMzkyTl5eVVcyYAAKCiSt+3S9/Hj6fGFR/5+fmSpMTExGrOBAAAVFZ+fr68Xu9x27isIiWKg3w+n77//ntFRUUpPz9fiYmJ2r17t6Kjo6s7tSqVl5dH30JQbe6bVLv7R99CE30LHWam/Px8JSQkKCzs+LM6atzIR1hYmFq0aCFJcrlckqTo6OhacWKCoW+hqTb3Tard/aNvoYm+hYZfG/EoxYRTAADgKIoPAADgqBpdfHg8Hk2aNEkej6e6U6ly9C001ea+SbW7f/QtNNG32qnGTTgFAAC1W40e+QAAALUPxQcAAHAUxQcAAHAUxQcAAHAUxQcAAHBUjS4+Zs6cqVatWikiIkK9evXSxx9/XN0pVdqHH36owYMHKyEhQS6XS4sXLw7Yb2a655571KxZM0VGRiotLU3btm2rnmQraerUqerRo4eioqIUGxuroUOHauvWrQFtjhw5ooyMDDVu3FgNGjTQRRddpOzs7GrKuOJmzZqlLl26+K88mJqaqnfeece/P1T7dawHH3xQLpdLY8eO9cdCuW/33nuvXC5XwNa+fXv//lDumyTt2bNHV155pRo3bqzIyEh17txZ69ev9+8P1deTVq1alTlvLpdLGRkZkkL7vJWUlGjixIlq3bq1IiMj1bZtWz3wwAMBP74WquftpFgNNX/+fHO73fbCCy/YZ599ZjfccIPFxMRYdnZ2dadWKW+//bb95S9/sYULF5okW7RoUcD+Bx980Lxery1evNg++eQT++Mf/2itW7e2w4cPV0/ClZCenm5z5syxLVu22KZNm+x//ud/rGXLlnbw4EF/m5tuuskSExMtMzPT1q9fb2effbb17t27GrOumDfeeMPeeust++qrr2zr1q129913W926dW3Lli1mFrr9+qWPP/7YWrVqZV26dLHbbrvNHw/lvk2aNMk6duxoe/fu9W/79+/37w/lvv3444+WlJRko0aNsrVr19o333xjy5Yts6+//trfJlRfT/bt2xdwzpYvX26SbMWKFWYW2udt8uTJ1rhxY1uyZInt2LHDFixYYA0aNLDHH3/c3yZUz9vJqLHFR8+ePS0jI8N/u6SkxBISEmzq1KnVmNXJObb48Pl8Fh8fb4888og/lpOTYx6Px/75z39WQ4YnZ9++fSbJVq5caWY/96Vu3bq2YMECf5svvvjCJNnq1aurK80T1rBhQ3vuuedqRb/y8/PttNNOs+XLl1vfvn39xUeo923SpEl25plnBt0X6n2766677Jxzzil3f216Pbntttusbdu25vP5Qv68DRo0yK699tqA2LBhw+yKK64ws9p13iqjRn7tUlRUpA0bNigtLc0fCwsLU1pamlavXl2NmVWtHTt2KCsrK6CfXq9XvXr1Csl+5ubmSpIaNWokSdqwYYOKi4sD+te+fXu1bNkypPpXUlKi+fPnq6CgQKmpqbWiXxkZGRo0aFBAH6Tacc62bdumhIQEtWnTRldccYV27dolKfT79sYbb6h79+665JJLFBsbq65du+rZZ5/1768trydFRUV65ZVXdO2118rlcoX8eevdu7cyMzP11VdfSZI++eQTffTRRxo4cKCk2nPeKqvG/aqtJB04cEAlJSWKi4sLiMfFxenLL7+spqyqXlZWliQF7WfpvlDh8/k0duxY9enTR506dZL0c//cbrdiYmIC2oZK/zZv3qzU1FQdOXJEDRo00KJFi5ScnKxNmzaFdL/mz5+v//73v1q3bl2ZfaF+znr16qW5c+fqjDPO0N69e3Xffffp3HPP1ZYtW0K+b998841mzZqlcePG6e6779a6des0ZswYud1ujRw5sta8nixevFg5OTkaNWqUpND/mxw/frzy8vLUvn17hYeHq6SkRJMnT9YVV1whqXa9D1RGjSw+EHoyMjK0ZcsWffTRR9WdSpU544wztGnTJuXm5uq1117TyJEjtXLlyupO66Ts3r1bt912m5YvX66IiIjqTqfKlX6alKQuXbqoV69eSkpK0quvvqrIyMhqzOzk+Xw+de/eXVOmTJEkde3aVVu2bNHs2bM1cuTIas6u6jz//PMaOHCgEhISqjuVKvHqq6/qH//4h+bNm6eOHTtq06ZNGjt2rBISEmrVeausGvm1S5MmTRQeHl5mNnN2drbi4+OrKauqV9qXUO/n6NGjtWTJEq1YsUItWrTwx+Pj41VUVKScnJyA9qHSP7fbrXbt2qlbt26aOnWqzjzzTD3++OMh3a8NGzZo3759Ouuss1SnTh3VqVNHK1eu1BNPPKE6deooLi4uZPsWTExMjE4//XR9/fXXIX3eJKlZs2ZKTk4OiHXo0MH/tVJteD3ZuXOn3nvvPV1//fX+WKiftzvuuEPjx4/XZZddps6dO+uqq67S7bffrqlTp0qqHeftRNTI4sPtdqtbt27KzMz0x3w+nzIzM5WamlqNmVWt1q1bKz4+PqCfeXl5Wrt2bUj008w0evRoLVq0SO+//75at24dsL9bt26qW7duQP+2bt2qXbt2hUT/juXz+VRYWBjS/erfv782b96sTZs2+bfu3bvriiuu8P87VPsWzMGDB7V9+3Y1a9YspM+bJPXp06fMUvavvvpKSUlJkkL/9USS5syZo9jYWA0aNMgfC/XzdujQIYWFBb7VhoeHy+fzSaod5+2EVPeM1/LMnz/fPB6PzZ071z7//HO78cYbLSYmxrKysqo7tUrJz8+3jRs32saNG02STZs2zTZu3Gg7d+40s5+XWMXExNjrr79un376qQ0ZMiRklljdfPPN5vV67YMPPghYJnfo0CF/m5tuuslatmxp77//vq1fv95SU1MtNTW1GrOumPHjx9vKlSttx44d9umnn9r48ePN5XLZu+++a2ah269gfrnaxSy0+/bnP//ZPvjgA9uxY4etWrXK0tLSrEmTJrZv3z4zC+2+ffzxx1anTh2bPHmybdu2zf7xj39YvXr17JVXXvG3CeXXk5KSEmvZsqXdddddZfaF8nkbOXKkNW/e3L/UduHChdakSRO78847/W1C+bydqBpbfJiZzZgxw1q2bGlut9t69uxpa9asqe6UKm3FihUmqcw2cuRIM/t5mdXEiRMtLi7OPB6P9e/f37Zu3Vq9SVdQsH5Jsjlz5vjbHD582G655RZr2LCh1atXzy688ELbu3dv9SVdQddee60lJSWZ2+22pk2bWv/+/f2Fh1no9iuYY4uPUO7b8OHDrVmzZuZ2u6158+Y2fPjwgOtghHLfzMzefPNN69Spk3k8Hmvfvr0988wzAftD+fVk2bJlJilovqF83vLy8uy2226zli1bWkREhLVp08b+8pe/WGFhob9NKJ+3E+Uy+8Vl1gAAAE6xGjnnAwAA1F4UHwAAwFEUHwAAwFEUHwAAwFEUHwAAwFEUHwAAwFEUHwAAwFEUHwAAwFEUHwAAwFEUHwAAwFEUHwAAwFH/D4Un4YpUzM4cAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import gym\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple, deque\n",
    "from itertools import count\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "\n",
    "# gym에서 wrappers option을 설정하면 영상 저장이 가능하다.\n",
    "env = gym.make('CartPole-v0', render_mode='rgb_array').unwrapped\n",
    "\n",
    "# matplotlib 설정\n",
    "is_ipython = 'inline' in matplotlib.get_backend()\n",
    "if is_ipython:\n",
    "    from IPython import display\n",
    "\n",
    "# interactive-on, 그때 그때 plot을 갱신하는 option\n",
    "plt.ion()\n",
    "\n",
    "# device 설정\n",
    "# GPU를 사용할 수 있으면 사용하고, 아니면 CPU를 사용한다.\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "Transition = namedtuple('Transition',\n",
    "                        ('state', 'action', 'next_state', 'reward'))\n",
    "\n",
    "\n",
    "# ReplayMemory를 정의\n",
    "class ReplayMemory(object):\n",
    "    def __init__(self, capacity):\n",
    "        # deque는 양방향 queue를 의미한다.\n",
    "        self.memory = deque([],maxlen=capacity)\n",
    "\n",
    "    def push(self, *args):\n",
    "        # Transition을 저장하는 부분이다.\n",
    "        self.memory.append(Transition(*args))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        # memory로부터 batch_size 길이 만큼의 list를 반환한다.\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        # memory의 길이를 반환한다.\n",
    "        return len(self.memory)\n",
    "    \n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, h, w, outputs):\n",
    "        super(DQN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size=5, stride=2)\n",
    "        self.bn1 = nn.BatchNorm2d(16)\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=5, stride=2)\n",
    "        self.bn2 = nn.BatchNorm2d(32)\n",
    "        self.conv3 = nn.Conv2d(32, 32, kernel_size=5, stride=2)\n",
    "        self.bn3 = nn.BatchNorm2d(32)\n",
    "\n",
    "        # Linear layer input의 차원을 맞춰주기 위한 함수이다.\n",
    "        # Conv2d layer의 연산 과정을 생각하여 size를 계산한다.\n",
    "        def conv2d_size_out(size, kernel_size = 5, stride = 2):\n",
    "            return (size - (kernel_size - 1) - 1) // stride  + 1\n",
    "        convw = conv2d_size_out(conv2d_size_out(conv2d_size_out(w)))\n",
    "        convh = conv2d_size_out(conv2d_size_out(conv2d_size_out(h)))\n",
    "        linear_input_size = convw * convh * 32\n",
    "        # 계산한 size에 output channel 개수인 32를 곱해 차원을 맞춰준다.\n",
    "        self.head = nn.Linear(linear_input_size, outputs)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.to(device)\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = F.relu(self.bn2(self.conv2(x)))\n",
    "        x = F.relu(self.bn3(self.conv3(x)))\n",
    "        rst = self.head(x.view(x.size(0), -1))\n",
    "        # 아래와 같은 형식의 output을 얻을 수 있다. (선택지 2개)\n",
    "        # rst >> tensor([[2.0840, 2.0001]])\n",
    "        return rst\n",
    "    \n",
    "resize = T.Compose([T.ToPILImage(),\n",
    "                    T.Resize(40, interpolation=T.InterpolationMode.BICUBIC),\n",
    "                    T.ToTensor()])\n",
    "\n",
    "def get_cart_location(screen_width):\n",
    "    world_width = env.x_threshold * 2\n",
    "    scale = screen_width / world_width\n",
    "    return int(env.state[0] * scale + screen_width / 2.0)  # MIDDLE OF CART\n",
    "\n",
    "def get_screen():\n",
    "    # gym이 요청한 화면은 400x600x3 이지만, 가끔 800x1200x3 처럼 큰 경우가 있습니다.\n",
    "    # 이것을 Torch order (CHW)로 변환한다.\n",
    "    screen = env.render().transpose((2, 0, 1))\n",
    "    # 카트는 아래쪽에 있으므로 화면의 상단과 하단을 제거하십시오.\n",
    "    _, screen_height, screen_width = screen.shape\n",
    "    screen = screen[:, int(screen_height*0.4):int(screen_height * 0.8)]\n",
    "    view_width = int(screen_width * 0.6)\n",
    "    cart_location = get_cart_location(screen_width)\n",
    "    if cart_location < view_width // 2:\n",
    "        slice_range = slice(view_width)\n",
    "    elif cart_location > (screen_width - view_width // 2):\n",
    "        slice_range = slice(-view_width, None)\n",
    "    else:\n",
    "        slice_range = slice(cart_location - view_width // 2,\n",
    "                            cart_location + view_width // 2)\n",
    "    # 카트를 중심으로 정사각형 이미지가 되도록 가장자리를 제거하십시오.\n",
    "    screen = screen[:, :, slice_range]\n",
    "    # float 으로 변환하고,  rescale 하고, torch tensor 로 변환하십시오.\n",
    "    # (이것은 복사를 필요로하지 않습니다)\n",
    "    screen = np.ascontiguousarray(screen, dtype=np.float32) / 255\n",
    "    screen = torch.from_numpy(screen)\n",
    "    # 크기를 수정하고 배치 차원(BCHW)을 추가하십시오.\n",
    "    return resize(screen).unsqueeze(0)\n",
    "\n",
    "# 예제 patch를 출력해볼 수 있는 부분\n",
    "env.reset()\n",
    "plt.figure()\n",
    "plt.imshow(get_screen().cpu().squeeze(0).permute(1, 2, 0).numpy(),\n",
    "           interpolation='none')\n",
    "plt.title('Example extracted screen')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "GAMMA = 0.999\n",
    "EPS_START = 0.9\n",
    "EPS_END = 0.05\n",
    "EPS_DECAY = 200\n",
    "TARGET_UPDATE = 10\n",
    "\n",
    "# initial screen을 설정한다.\n",
    "init_screen = get_screen()\n",
    "\n",
    "# init_screen.shape >> torch.Size([1, 3, 40, 90])\n",
    "_, _, screen_height, screen_width = init_screen.shape\n",
    "\n",
    "# gym의 action space에서 action의 가짓수를 얻는다.\n",
    "# n_actions >> 2\n",
    "n_actions = env.action_space.n\n",
    "\n",
    "# 여기서 network의 구성은 DQN(40, 90, 2) 이다.\n",
    "policy_net = DQN(screen_height, screen_width, n_actions).to(device)\n",
    "target_net = DQN(screen_height, screen_width, n_actions).to(device)\n",
    "\n",
    "# policy network의 network parameter를 불러온다.   \n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "target_net.eval()\n",
    "\n",
    "optimizer = optim.RMSprop(policy_net.parameters())\n",
    "\n",
    "# Capacity (즉, maximum length) 10000짜리 deque 이다.\n",
    "memory = ReplayMemory(10000)\n",
    "steps_done = 0\n",
    "\n",
    "def select_action(state):\n",
    "    # Global 변수로 선언한다.\n",
    "    global steps_done\n",
    "    # random.random() >> [0.0, 1.0) 구간의 소수점 숫자를 반환한다.\n",
    "    sample = random.random()\n",
    "    # steps_done이 커짐에 따라 epsilon 값이 줄어든다. \n",
    "    eps_threshold = EPS_END + (EPS_START - EPS_END) * \\\n",
    "    math.exp(-1. * steps_done / EPS_DECAY)\n",
    "    steps_done += 1\n",
    "\n",
    "    if sample > eps_threshold:\n",
    "        with torch.no_grad():\n",
    "            # 기댓값이 더 큰 action을 고르자. \n",
    "            # 바로 예를 들어 설명해보면, 아래의 논리로 코드가 진행된다.\n",
    "            '''\n",
    "            policy_net(state) >> tensor([[0.5598, 0.0144]])\n",
    "            policy_net(state).max(1) >> ('max value', 'max 값의 index')\n",
    "            policy_net(state).max(1)[1] >> index를 선택함.\n",
    "            policy_net(state).max(1)[1].view(1, 1) >> tensor([[0]]) \n",
    "            '''\n",
    "            # 즉, 위 예제의 경우 index 0에 해당하는 action을 선택하는 것이다.\n",
    "            return policy_net(state).max(1)[1].view(1, 1)\n",
    "    else:\n",
    "        # tensor([['index']])의 형식으로 random하게 action이 선택된다. \n",
    "        # 즉, 0 이나 1 값이 선택됨.\n",
    "        return torch.tensor([[random.randrange(n_actions)]], device=device, \\\n",
    "        dtype=torch.long)\n",
    "episode_durations = []\n",
    "\n",
    "def plot_durations():\n",
    "    plt.figure(2)\n",
    "    plt.clf()\n",
    "    durations_t = torch.tensor(episode_durations, dtype=torch.float)\n",
    "    plt.title('Training...')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Duration')\n",
    "    plt.plot(durations_t.numpy())\n",
    "    # 100개 에피소드의 평균을 가져 와서 도표 그리기\n",
    "    if len(durations_t) >= 100:\n",
    "        means = durations_t.unfold(0, 100, 1).mean(1).view(-1)\n",
    "        means = torch.cat((torch.zeros(99), means))\n",
    "        plt.plot(means.numpy())\n",
    "\n",
    "    plt.pause(0.001)  # 도표가 업데이트되도록 잠시 멈춤\n",
    "    if is_ipython:\n",
    "        display.clear_output(wait=True)\n",
    "        display.display(plt.gcf())\n",
    "        \n",
    "def optimize_model():\n",
    "    if len(memory) < BATCH_SIZE:\n",
    "        return\n",
    "\n",
    "    # 여기서부터는 memory의 길이 (크기)가 BATCH_SIZE 이상인 경우이다.\n",
    "    # BATCH_SIZE의 크기만큼 sampling을 진행한다.  \n",
    "    transitions = memory.sample(BATCH_SIZE)\n",
    "\n",
    "    # Remind) Transition = namedtuple('Transition', ('state', 'action', 'next_state', 'reward'))\n",
    "    # 아래의 코드를 통해 batch에는 각 항목 별로 BATCH_SIZE 개수 만큼의 성분이 한번에 묶여 저장된다.\n",
    "    batch = Transition(*zip(*transitions))\n",
    "\n",
    "    # 우선 lambda가 포함된 line의 빠른 이해를 위해 다음의 예제를 보자. \n",
    "    # list(map(lambda x: x ** 2, range(5))) >> [0, 1, 4, 9, 16]\n",
    "    '''\n",
    "    즉, 아래의 line을 통해 BATCH_SIZE 개의 원소를 가진 tensor가 구성된다.  \n",
    "    또한 각 원소는 True와 False 로 구성되어 있다. \n",
    "    batch.next_state는 다음 state 값을 가지고 있는 tensor로 크게 두 부류로 구성된다.\n",
    "    >> None 혹은 torch.Size([1, 3, 40, 90]) 의 형태\n",
    "    '''\n",
    "    # 정리하면 아래의 코드는 batch.next_state에서 None을 갖는 원소를 False로, \n",
    "    # 그렇지 않으면 True를 matching 시키는 line이다.\n",
    "    non_final_mask = torch.tensor(tuple(map(lambda s: s is not None,\n",
    "                                          batch.next_state)), device=device, dtype=torch.bool)\n",
    "    \n",
    "    # batch.next_state의 원소들 중 next state가 None이 아닌 원소들의 집합이다. \n",
    "    # torch.Size(['next_state가 None이 아닌 원소의 개수', 3, 40, 90])의 형태\n",
    "    non_final_next_states = torch.cat([s for s in batch.next_state\n",
    "                                                if s is not None])\n",
    "\n",
    "    # 아래 세 변수의 size는 모두 torch.Size([128, 3, 40, 90]) 이다. \n",
    "    state_batch = torch.cat(batch.state)\n",
    "    action_batch = torch.cat(batch.action)\n",
    "    reward_batch = torch.cat(batch.reward)\n",
    "\n",
    "    # torch.gather(input, dim, index, *, sparse_grad=False, out=None) → Tensor\n",
    "    # action_batch 에 들어있는 0 혹은 1 값으로 index를 설정하여 결과값에서 가져온다.\n",
    "    # 즉, action_batch 값에 해당하는 결과 값을 불러온다.  \n",
    "    state_action_values = policy_net(state_batch).gather(1, action_batch)\n",
    "\n",
    "    # 한편, non_final_next_states의 행동들에 대한 기대값은 \"이전\" target_net을 기반으로 계산됩니다.\n",
    "\n",
    "    # 일단 모두 0 값을 갖도록 한다.  \n",
    "    next_state_values = torch.zeros(BATCH_SIZE, device=device)\n",
    "\n",
    "    # non_final_mask에서 True 값을 가졌던 원소에만 값을 넣을 것이고, False 였던 원소에게는 0 값을 유지할 것이다. \n",
    "    # target_net(non_final_next_states).max(1)[0].detach() 를 하면, \n",
    "    # True 값을 갖는 원소의 개수만큼 max value 값이 모인다.  \n",
    "    # 이들을 True 값의 index 위치에만 반영시키도록 하자.\n",
    "    # 정리하면 한 state에서 더 큰 action을 선택한 것에 대한 value 값이 담기게 된다.  \n",
    "    next_state_values[non_final_mask] = target_net(non_final_next_states).max(1)[0].detach()\n",
    "\n",
    "    # expected Q value를 계산하자.  \n",
    "    expected_state_action_values = (next_state_values * GAMMA) + reward_batch\n",
    "\n",
    "    # Huber Loss 계산\n",
    "    criterion = nn.SmoothL1Loss()\n",
    "    loss = criterion(state_action_values, expected_state_action_values.unsqueeze(1))\n",
    "\n",
    "    # Optimize parameters\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    for param in policy_net.parameters():\n",
    "        # 모든 원소를 [ min, max ]의 범위로 clamp\n",
    "        param.grad.data.clamp_(-1, 1)\n",
    "    optimizer.step()\n",
    "    \n",
    "# batch = Transition(*zip(*transitions)) \n",
    "\n",
    "# state_action_values = policy_net(state_batch).gather(1, action_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20 회 완료\n",
      "Complete\n"
     ]
    }
   ],
   "source": [
    "num_episodes = 20\n",
    "for i_episode in range(num_episodes):\n",
    "    # env와 state를 초기화 한다.  \n",
    "    env.reset()\n",
    "    last_screen = get_screen()\n",
    "    current_screen = get_screen()\n",
    "    state = current_screen - last_screen\n",
    "    \n",
    "    # 여기서 사용한 count()는 from itertools import count 로 import 한 것이다. \n",
    "    # t -> 0, 1, 2, ... 의 순서로 진행된다.  \n",
    "    for t in count():\n",
    "        # state shape >> torch.Size([1, 3, 40, 90]) \n",
    "        # action result >> tensor([[0]]) or tensor([[1]])\n",
    "        action = select_action(state)\n",
    "\n",
    "        # 선택한 action을 대입하여 reward와 done을 얻어낸다. \n",
    "        # env.step(action.item())의 예시  \n",
    "        # >> (array([-0.008956, -0.160571,  0.005936,  0.302326]), 1.0, False, {})\n",
    "        _, reward, done, _, _ = env.step(action.item())\n",
    "        reward = torch.tensor([reward], device=device)\n",
    "\n",
    "        # 새로운 state를 구해보자.  \n",
    "        last_screen = current_screen\n",
    "        # action이 반영된 screen을 얻어낸다. \n",
    "        current_screen = get_screen()\n",
    "\n",
    "        if not done:\n",
    "            next_state = current_screen - last_screen\n",
    "        else: # 만약, done이 True라면 그만하자. \n",
    "            next_state = None\n",
    "\n",
    "        # 얻어낸 transition set을 memory에 저장\n",
    "        memory.push(state, action, next_state, reward)\n",
    "\n",
    "        # 다음 상태로 이동\n",
    "        state = next_state\n",
    "\n",
    "        # (policy network에서) 최적화 한단계 수행\n",
    "        optimize_model()\n",
    "\n",
    "        # 마찬가지로 done이 True 라면,\n",
    "        if done:\n",
    "            # 하나의 episode가 몇 번 진행 되었는지 counting 하는 line\n",
    "            episode_durations.append(t + 1)\n",
    "            plot_durations()\n",
    "            plt.show()\n",
    "            break\n",
    "    \n",
    "    # TARGET_UPDATE 마다 target network의 parameter를 update 한다. \n",
    "    if i_episode % TARGET_UPDATE == 0:\n",
    "        target_net.load_state_dict(policy_net.state_dict())\n",
    "\n",
    "    # (episode 한번에 대한) 전체 for문 1회 종료.\n",
    "    print(i_episode+1,'회 완료')\n",
    "\n",
    "# 학습 마무리. \n",
    "print('Complete')\n",
    "env.render()\n",
    "env.close()\n",
    "plt.ioff()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jakek\\anaconda3\\envs\\proj4\\lib\\site-packages\\keras\\optimizers\\legacy\\adam.py:117: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"c:\\Users\\jakek\\anaconda3\\envs\\proj4\\lib\\site-packages\\keras\\engine\\training.py\", line 2169, in predict_function  *\n        return step_function(self, iterator)\n    File \"c:\\Users\\jakek\\anaconda3\\envs\\proj4\\lib\\site-packages\\keras\\engine\\training.py\", line 2155, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"c:\\Users\\jakek\\anaconda3\\envs\\proj4\\lib\\site-packages\\keras\\engine\\training.py\", line 2143, in run_step  **\n        outputs = model.predict_step(data)\n    File \"c:\\Users\\jakek\\anaconda3\\envs\\proj4\\lib\\site-packages\\keras\\engine\\training.py\", line 2111, in predict_step\n        return self(x, training=False)\n    File \"c:\\Users\\jakek\\anaconda3\\envs\\proj4\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"c:\\Users\\jakek\\anaconda3\\envs\\proj4\\lib\\site-packages\\keras\\engine\\input_spec.py\", line 253, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Exception encountered when calling layer 'sequential_8' (type Sequential).\n    \n    Input 0 of layer \"dense_24\" is incompatible with the layer: expected min_ndim=2, found ndim=1. Full shape received: (None,)\n    \n    Call arguments received by layer 'sequential_8' (type Sequential):\n      • inputs=tf.Tensor(shape=(None,), dtype=float32)\n      • training=False\n      • mask=None\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[34], line 72\u001b[0m\n\u001b[0;32m     70\u001b[0m     \u001b[39mbreak\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(agent\u001b[39m.\u001b[39mmemory) \u001b[39m>\u001b[39m batch_size:\n\u001b[1;32m---> 72\u001b[0m     agent\u001b[39m.\u001b[39;49mreplay(batch_size)\n",
      "Cell \u001b[1;32mIn[34], line 42\u001b[0m, in \u001b[0;36mDQNAgent.replay\u001b[1;34m(self, batch_size)\u001b[0m\n\u001b[0;32m     40\u001b[0m target \u001b[39m=\u001b[39m reward\n\u001b[0;32m     41\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m done:\n\u001b[1;32m---> 42\u001b[0m     target \u001b[39m=\u001b[39m reward \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgamma \u001b[39m*\u001b[39m np\u001b[39m.\u001b[39mamax(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel\u001b[39m.\u001b[39;49mpredict(next_state)[\u001b[39m0\u001b[39m])\n\u001b[0;32m     43\u001b[0m target_f \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mpredict(state)\n\u001b[0;32m     44\u001b[0m target_f[\u001b[39m0\u001b[39m][action] \u001b[39m=\u001b[39m target\n",
      "File \u001b[1;32mc:\\Users\\jakek\\anaconda3\\envs\\proj4\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_file23u9uxtg.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__predict_function\u001b[1;34m(iterator)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     14\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m     retval_ \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(step_function), (ag__\u001b[39m.\u001b[39mld(\u001b[39mself\u001b[39m), ag__\u001b[39m.\u001b[39mld(iterator)), \u001b[39mNone\u001b[39;00m, fscope)\n\u001b[0;32m     16\u001b[0m \u001b[39mexcept\u001b[39;00m:\n\u001b[0;32m     17\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: in user code:\n\n    File \"c:\\Users\\jakek\\anaconda3\\envs\\proj4\\lib\\site-packages\\keras\\engine\\training.py\", line 2169, in predict_function  *\n        return step_function(self, iterator)\n    File \"c:\\Users\\jakek\\anaconda3\\envs\\proj4\\lib\\site-packages\\keras\\engine\\training.py\", line 2155, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"c:\\Users\\jakek\\anaconda3\\envs\\proj4\\lib\\site-packages\\keras\\engine\\training.py\", line 2143, in run_step  **\n        outputs = model.predict_step(data)\n    File \"c:\\Users\\jakek\\anaconda3\\envs\\proj4\\lib\\site-packages\\keras\\engine\\training.py\", line 2111, in predict_step\n        return self(x, training=False)\n    File \"c:\\Users\\jakek\\anaconda3\\envs\\proj4\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"c:\\Users\\jakek\\anaconda3\\envs\\proj4\\lib\\site-packages\\keras\\engine\\input_spec.py\", line 253, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Exception encountered when calling layer 'sequential_8' (type Sequential).\n    \n    Input 0 of layer \"dense_24\" is incompatible with the layer: expected min_ndim=2, found ndim=1. Full shape received: (None,)\n    \n    Call arguments received by layer 'sequential_8' (type Sequential):\n      • inputs=tf.Tensor(shape=(None,), dtype=float32)\n      • training=False\n      • mask=None\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import random\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "class DQNAgent:\n",
    "    def __init__(self, state_size, action_size):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.memory = []\n",
    "        self.gamma = 0.95  # discount rate\n",
    "        self.epsilon = 1.0  # exploration rate\n",
    "        self.epsilon_decay = 0.995\n",
    "        self.epsilon_min = 0.01\n",
    "        self.learning_rate = 0.001\n",
    "        self.model = self.build_model()\n",
    "\n",
    "    def build_model(self):\n",
    "        model = Sequential()\n",
    "        model.add(Dense(24, input_dim=self.state_size, activation='relu'))\n",
    "        model.add(Dense(24, activation='relu'))\n",
    "        model.add(Dense(self.action_size, activation='linear'))\n",
    "        model.compile(loss='mse', optimizer=Adam(lr=self.learning_rate))\n",
    "        return model\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def act(self, state):\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return random.randrange(self.action_size)\n",
    "        act_values = self.model.predict(state)\n",
    "        return np.argmax(act_values[0])\n",
    "\n",
    "    def replay(self, batch_size):\n",
    "        minibatch = random.sample(self.memory, batch_size)\n",
    "        for state, action, reward, next_state, done in minibatch:\n",
    "            target = reward\n",
    "            if not done:1\n",
    "                target = reward + self.gamma * np.amax(self.model.predict(next_state)[0])\n",
    "            target_f = self.model.predict(state)\n",
    "            target_f[0][action] = target\n",
    "            self.model.fit(state, target_f, epochs=1, verbose=0)\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    env = gym.make('CartPole-v1')\n",
    "    state_size = env.observation_space.shape[0]\n",
    "    action_size = env.action_space.n\n",
    "    agent = DQNAgent(state_size, action_size)\n",
    "\n",
    "    batch_size = 32\n",
    "    episodes = 1000\n",
    "    for e in range(episodes):\n",
    "        state = env.reset()[0]\n",
    "        # state = np.reshape(state, [1, state_size])\n",
    "        for time in range(500):\n",
    "            action = agent.act(state)\n",
    "            next_state, reward, done, _,_ = env.step(action)\n",
    "            reward = reward if not done else -10\n",
    "            next_state = next_state\n",
    "            agent.remember(state, action, reward, next_state, done)\n",
    "            state = next_state\n",
    "            if done:\n",
    "                print(\"episode: {}/{}, score: {}, e: {:.2}\"\n",
    "                      .format(e, episodes, time, agent.epsilon))\n",
    "                break\n",
    "            if len(agent.memory) > batch_size:\n",
    "                agent.replay(batch_size)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3.8",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
